\chapter{Sampling distributions}
In this chapter, we briefly review some concepts from \textsc{math} 61.2 regarding the probability distributions of functions of random variables (now called statistics), using techniques like moment-generating functions and the cdf technique. 
We prove the Lindeberg-L\'evy form of the celebrated central limit theorem, as well as derive certain special distributions of sample means, sample variances, and order statistics. 
Finally, we introduce three new probability distributions, the \(\upchi^2\) distribution, the student \(t\) distrubution, and the \(\mathscr{F}\) distribution. 
\section{Distribution of the mean and the CLT}
Given \(n\) random variables, \(X_1,X_2,\ldots, X_n\), a \textbf{statistic} is a function \(g(X_1, X_2,\ldots, X_n)\) of these random variables. 
The probability distribution of a statistic is called its sampling distribution. 
\begin{example}
    Let \({(X_i)}_{i=1}^n\iid \mathrm{Be}(p)\). 
    Define \(T_n\) to be their sum, \(T_n = \sum_{i=1}^n X_i\). 
    Then, we know that \(T_n \sim \mathrm{Bin}(n,p)\). 
\end{example}
\begin{example}
    Let \(X_1,X_2\iid \mathscr{U}(0,1)\). Find the distribution of \(X_1 + X_2\).

    Note that the support for \(X_1 + X_2\) is the unit square in \(\mathbb{R}^2\). 
    The function \(x_1 + x_2 = t\) corresponds to a line moving diagonally across the square from left to right. 
    We use the cdf technique:\[
    F_{X_1+ X_2} (t) = \Pr[X_1 + X_2 \leq t] = \iint_R 1 \cdot \der x_1 \der x_2,~R = \{(x_1,x_2) \in [0,1] : x_1 + x_2 \leq t\}.
    \]Notice that we can split the integral into two cases: if (1) \(0< t<1\), then \(R\) is bounded by the two axes and \(x_1 + x_2 = t\); whereas if (2) \(1\leq t < 2\), then \(R\) is the unit square minus the upper right triangle bounded by \(x_2 = 1, x_1 = 1\), and \(x_1 + x_2 =t\). 
    This gives us:\begin{itemize}
        \item when \(0<t<1\): \[
        F_{X_1 + X_2} (t) = \int_0^t \int_0^{t-x_2} 1 \cdot \der x_1 \der x_2 = \int_0^t (t - x_2) \der x_2 = \biggl(tx_2 - \frac{x_2^2}{2} \bigg)\bigg|_0^t = t^2 - \frac{t^2}{2}=  \frac{t^2}{2}.\]
        \item when \(1\leq t < 2\): \begin{align*}
        F_{X_1 + X_2} (t) &= 1 - \int_{t-1}^1 \int_{t-x_2}^1 1\cdot \der x_1 \der x_2  = 1 - \int_{t-1}^1 (1-t + x_2) \der x_2 \\ 
        &= 1 - \bigg( x_2 (1-t) + \frac{x_2^2}{2} \bigg)\bigg|_{t-1}^1 = \frac{1}{2} + (t-1) - \frac{{(t-1)}^2}{2}.\end{align*}
    \end{itemize}
    This gives us the cdf of \(X_1 + X_2\). 
    To find its pdf, we simply differentiate and obtain\[
    p_{X_1 + X_2}(t) = \begin{cases}
        t, &\text{when}~0<t<1 \\ 
        2-t,&\text{when}~1\leq t <2
    \end{cases} \]as our desired pdf. 
\end{example}

Consider \({(X_i)}_{i=1}^n \iid \mathscr{N}(\mu, \sigma^2)\). 
Using moment-generating functions, we can determine the distribution of the so-called \textbf{sampling mean} \(\overline{X}_n = \frac{1}{n}\sum_{i=1}^n X_i\), a common statistic. 
First recall that \(\mathscr{M}_{\alpha X}(t) = \mathscr{M}_X(\alpha t)\), and \(\mathscr{M}_{\sum X_i}(t) = \prod_i \mathscr{M}_{X_i}(t)\) for a random variable \(X\). Now:\begin{align*}
    \mathscr{M}_{\overline{X}_n}(t) &= \mathscr{M}_{\frac{1}{n}\sum X_i}(t) = \mathscr{M}_{\sum X_i} \bigl(\tfrac{t}{n}\bigr) = \prod_{i=1}^n \mathscr{M}_{X_i} \big(\tfrac{t}{n}\big) \\
    &= {\biggl\{ \exp\bigg[\mu \biggl(\frac{t}{n}\bigg) + \frac{1}{2}\sigma^2 {\biggl(\frac{t}{n}\bigg)}^2\bigg]\biggr\} }^n = \exp \bigg[\, \mu t + \frac{1}{2}\biggl(\frac{\sigma^2}{n}\bigg)t^2\bigg],&\text{substituting}~\frac{t}{n}~\text{into}~\mathscr{M}_{X_i}(t)
\end{align*}which is the mgf of a normal random variable with mean \(\mu\) and variance \(\frac{\sigma^2}{n}\), so \(\overline{X}_n \sim \mathscr{N}(\mu, \frac{\sigma^2}{n})\). 
In general, recall also from \textsc{math} 61.2 that if \({(X_i)}_{i=1}^n\) are independent normal random variables with means \(\mu_i\) and variances \(\sigma_i^2\), then the distribution of a linear combination of them would obey a normal distribution with a similarly linear combination of their means and variances. 
Symbolically,\[
    {(X_i)}_{i=1}^n \overset{\mathrm{ind}}{\sim} \mathscr{N} (\mu_i, \sigma_i^2) \implies \sum_{i=1}^n a_i X_i \sim \mathscr{N}\Big(\sum_{i=1}^n a_i \mu_i, \sum_{i=1}^n a_i^2 \sigma_i^2\Big). 
\]This particular quality of the sum of normal random variables being normally distributed is the \textbf{infinite divisibility} of the normal distribution. 
Other distributions that obey this law are the gamma distribution and the Poisson distribution. 

\begin{example}
    Compute the probability that the sample mean of size 10 taken from a normal population with mean 1 and variance 2 has a value between 1.2 and 3.1. 

    We know that the sample mean \(\overline{X}_{10} \sim \mathscr{N}(1, \frac{2}{10})\). 
    Thus,\begin{align*}
    \Pr [1.2 <\overline{X}_{10} <3.1] &= \code{pnorm(3.1,mean=1,sd=sqrt(0.2))} \\ &\phantom{=}- \code{pnorm(1.2,mean=1,sd=sqrt(0.2))} \approx 0.3273,\end{align*}
    using \R~commands. 
\end{example}

\begin{minipage}{.14\textwidth}
    \includegraphics[width=2cm]{nerd_maddy.png} 
\end{minipage}%
\fbox{
\begin{minipage}[t]{.76\textwidth}
    \textbf{Nerd Interjection!} Here's a brief anatomy of an \R~command:\[
    \code{\{p|d|q\}\{distribution\}(value, params)}\]\begin{itemize}
        \item \(\code{\{p|d|q\}}\): Using \(\code{p}\) will return the cdf of the distribution evaluated at \(\code{value}\), i.e., \(\Pr[X<\code{value}]\). Using \(\code{d}\) will return the result of evaluating the pdf instead (useful for discrete distributions). Finally, using \(\code{q}\) will evaluate the inverse cdf, i.e., the solution to the equation \(\Pr[X<x] = \code{value}\). 
        \item \(\code{distribution}\): Pretty self-explanatory. Names are may be shortened if long (e.g., \(\code{norm}\) for normal and \(\code{binom}\) for binomial). 
        \item \(\code{params}\): In general, the parameters you need to define the distribution will appear in the usual order that they do and you don't really need the whole \(\code{mean=}\) (e.g., \(\code{pnorm(0.5,0,1)}\) is standard normal and returns 0). 
    \end{itemize}
\end{minipage}}
\vspace{10pt}

In practice, though, it might be that we don't know the underlying distribution of a sample \({(X_i)}_{i=1}^n\). 
This problem arises frequently when dealing with real data, which does not, in general, follow an explicitly given probability distribution. 
However, we have the following result: 

\begin{theorem}[Lindeberg-L\'evy CLT]
    Suppose \({(X_i)}_{i=1}^n\) are i.i.d.\ random variables obeying an unknown probability distribution with mean \(\mu\) and variance \(\sigma^2\). 
    Then, \[\overline{X}_n \overset{\mathrm{d}}{\longrightarrow} \mathscr{N}(\mu, \tfrac{\sigma^2}{n}),\]
    where \(\overset{\mathrm{d}}{\longrightarrow}\) indicates the distribution approaching \(\mathscr{N}(\mu, \frac{\sigma^2}{n})\) as \(n\to\infty\).
\end{theorem}

\begin{proof}
    To prove this, we will use mgf's. 
    Let \(\mathscr{M}_{X}(t)\) be the mgf of one of these \(X_i\)'s. 
    To show that \(\mathscr{M}_{\overline{X}_n}(t) \) approaches the mgf of a normal random variable with mean \(\mu\) and variance \(\frac{\sigma^2}{n}\) as \(n\to\infty\), it suffices to show that\begin{equation}\label{eq:1-1}
        \lim_{n\to\infty} \mathscr{M}_Z(t) = \e^{t^2 / 2},~\text{where}~Z= \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}},~\text{the standardized form of}~\overline{X}_n. 
    \end{equation}
    First, we manipulate \(Z\) to make it resemble a linear combination of the \(X_i\)'s:\[
    \frac{\overline{X}_n - \mu}{\sigma / \sqrt{n}} = \frac{\sqrt{n}}{\sigma} \bigg(\frac{1}{n}\sum_{i=1}^n (X_i - \mu)\bigg) = \frac{1}{\sigma\sqrt{n}} \sum_{i=1}^n (X_i - \mu). 
    \]Substituting this into~\eqref{eq:1-1}~gives us\[
    \mathscr{M}_Z(t) = \mathscr{M}_{\sum (X_i - \mu)}\big(\tfrac{t}{\sigma \sqrt{n}}\big) = \prod_{i=1}^n \mathscr{M}_{X_i - \mu}\big(\tfrac{t}{\sigma\sqrt{n}}\big) = {\Big[\mathscr{M}_{X_i - \mu}\big(\tfrac{t}{\sigma\sqrt{n}}\big)\Big]}^n.\]
    Since we are working with exponents to the \(n\), it might be helpful to take the logarithm of this expression. 
    Hence, notice that to prove~\eqref{eq:1-1}, it further suffices to show\[
    \lim_{n\to\infty} \ln \mathscr{M}_Z(t) = \frac{t^2}{2}. 
    \]As a final simplification, we can make a substitution letting \(h\coloneq \frac{t}{\sigma\sqrt{n}}\), so that \(n = \frac{t^2}{\sigma^2 h^2}\) and \(h\to 0\) as \(n\to \infty\). 
    Then:\begin{align*}
    \lim_{n\to\infty} \ln \mathscr{M}_Z(t) &= \lim_{n\to\infty} \ln {\Big[\mathscr{M}_{X_i - \mu}\big(\tfrac{t}{\sigma\sqrt{n}}\big)\Big]}^n = \lim_{n\to\infty} n\cdot \ln \Big(\mathscr{M}_{X_i -\mu}\big(\tfrac{t}{\sigma\sqrt{n}}\big) \Big)\\
    &= \lim_{h\to 0} \frac{t^2}{\sigma^2 h^2} \cdot \ln \big(\mathscr{M}_{X_i - \mu}(h)\big) =\frac{t^2}{\sigma^2} \lim_{h\to 0} \frac{\ln\big(\mathscr{M}_{X_i - \mu}(h)\big)}{h^2} &\text{form}~\frac{0}{0},~\text{since}~\mathscr{M}_{X}(0) = 1~\text{for all r.v.'s}~X \\ 
    &=\frac{t^2}{\sigma^2}\lim_{h\to 0} \frac{\frac{1}{\mathscr{M}_{X_i -\mu}(h)} \cdot \mathscr{M}_{X_i - \mu}'(h)}{2h} = \frac{t^2}{\sigma^2} \lim_{h\to 0} \frac{\mathscr{M}_{X_i - \mu}'(h)}{2h\cdot \mathscr{M}_{X_i - \mu}(h)} &\text{form}~\frac{0}{0},~\text{since}~\mathscr{M}_{X_i -\mu}'(0) = \E[X_i - \mu] = 0 \\ 
    &=\frac{t^2}{\sigma^2}\lim_{h\to 0} \frac{\overbrace{\mathscr{M}_{X_i -\mu}''(h)}^{\E[{(X_i - \mu)}^2]~=~\var [X_i]~=~\sigma^2}}{\underbrace{2\cdot\mathscr{M}_{X_i -\mu}(h)}_2 + \underbrace{2h\cdot\mathscr{M}_{X_i -\mu}'(h)}_{0}} = \frac{t^2}{\sigma^2} \cdot \frac{\sigma^2}{2} = \frac{t^2}{2},
    \end{align*}as desired. 
\end{proof}

The central limit theorem (CLT) is often useful in deriving approximations of very large discrete properties, where finding exact values might be practically impossible due to combinatorial explosion. 
This can be seen in the following form of the CLT, which is a corollary of the Lindeberg-L\'evy CLT: 

\begin{theorem}[De Moivre-Laplace Theorem]
    If \((X_i)_{i=1}^n \iid \mathrm{Be}(p)\) (i.e., independent coin flips with probability \(p\)), then\[\sum_{i=1}^n X_i \sim \mathrm{Bin}(n,p) \approach \mathscr{N}\big(np,np(1-p)\big).\]
\end{theorem}

Thus, given a random variable \(X\) obeying a binomial distribution with very large \(n\), we can approximate\[
\Pr[X=k] = \Pr[k-\tfrac{1}{2} < X < k +\tfrac{1}{2}] \approx \Pr[k-\tfrac{1}{2} < W < k + \tfrac{1}{2}], 
\]where \(W\) is a normally distributed random variable with the appropriate parameters.
This approximation is valid whenever \(n> 9\cdot\frac{p}{1-p}\) and \(n> 9\cdot\frac{1-p}{p}\). 

\begin{example}
    Let \(X\sim \mathrm{Bin}(10, 0.5)\). Find \(\Pr[X=3]\) by approximating it first, then by finding the exact value. 

    Since \(n = 10 > 9\), we can use the normal approximation. Thus,\begin{align*}
        \Pr[X=3] &= \Pr[2.5 <X < 3.5] \approx \Pr[2.5<W<3.5] &\text{where}~W\sim\mathscr{N}(5,2.5) \\ 
        &= \code{pnorm(3.5, 5, sqrt(2.5))}- \code{pnorm(2.5, 5, sqrt(2.5))} \approx 0.114467. 
    \end{align*}By the usual pdf, we simply have:\[
    \Pr[X=3] = \binom{10}{3} {\biggl(\frac{1}{2}\biggr)}^3{\biggl(\frac{1}{2}\biggr)}^7 = \code{dbinom(3,10,0.5)} \approx 0.1171875,
    \]as desired. 
\end{example}

\vspace{10pt}
Perhaps a more extreme example of a combinatorial approximation is given by the following theorem:
\begin{theorem}[Stirling's approximation]
    For large values of \(n\),\[
    n! \sim \sqrt{2\uppi n}\cdot n^n \cdot\e^{-n}.\]
    That is, the limit of their ratio approaches 1 as \(n\) approaches infinity. 
\end{theorem}

\begin{example}
    Let \({(X_i)}_{i=1}^{64} \iid \mathscr{U}(0,1)\). 
    Find \(k\in\mathbb{R}\) such that \(\Pr \Big[\sqrt[64]{\textstyle\prod_{i=1}^{64} X_i} \geq k\Big] = 0.10\).

    Taking the logarithm of the expression inside the probability function gives us\[
    \Pr \bigg[\frac{1}{64}\sum_{i=1}^{64} \ln X_i \geq \ln k\bigg] = 0.10,\]
    which we can recognize as the sampling mean of random variables \({(\ln X_i)}_{i=1}^{64}\). 
    Ideally, then, we can apply the CLT to approximate the value of \(k\) that will make this statement true, since this will give us an approximation of the distribution of \(\overline{\ln X}_{64}\), but first we need to find \(\E[\ln X_i]\) and \(\var[\ln X_i]\). 
    We use the cdf technique. 
    Note that since \(X_i\) takes on values between 0 and 1, \(\ln X_i\) will take on values less than 0. 
    Hence\[
    F_{\,\ln X_i}(t) = \Pr[\ln X_i \leq t] = \Pr [X_i \leq \e^t] = \int_0^{\e^t} 1\cdot \der x_i, t<0,
    \]and so \(p_{\,\ln X_i}(t) = \e^t\), with \(t<0\). 
    From here we can see that \(\E[\ln X_i] = -1\) and \(\var[\ln X_i] = 1\). 
    Then, by the CLT, we have \(\frac{1}{64} \sum_{i=1}^{64} \ln X_i\approach \mathscr{N} (-1, \frac{1}{64})\). 
    Finally, we compute\begin{align*}
        \Pr \bigg[\frac{1}{64}\sum_{i=1}^{64} \ln X_i \geq \ln k\bigg]  &=0.10 \\ 
        1 - \Pr \bigg[\frac{1}{64}\sum_{i=1}^{64} \ln X_i \leq \ln k\bigg] &= 0.10 \\ 
        \ln k &= \code{qnorm(0.9,mean=-1,sd=1/8)}\\ 
        k&\approx \e^{-0.84},
    \end{align*}as desired.
\end{example} 

\section{Distribution of the sample variance} 
After the mean, perhaps the next most common statistic in considertaion is the \textbf{sampling variance}, defined as\[
    S_n^2 \coloneq \frac{1}{n-1} \sum_{i=1}^n {(X_i - \overline{X}_n)}^2.
\]Before we derive its distribution, however, it is necessary to introduce another distribution---a variant of one that we have already seen before---and prove some basic results about it. 

A random variable \(X\) obeys a \(\upchi^2\) distribution with \(k\) degrees of freedom  if it has the following pdf:\[
    f(x) = \frac{1}{\Gamma (\frac{k}{2}) \cdot 2^{\sfrac{k}{2}}} \cdot x^{\sfrac{k}{2} -1} \e^{-\sfrac{x}{2}},~\text{where}~x,k\in (0,\infty).  
\]Note that \(\upchi_k^2\) is equivalent to \(\mathrm{Gamma}(\frac{k}{2},2)\). 
Thus, we have the following properties, for \(X\sim \upchi_k^2\):\begin{enumerate}
    \item \(\E[X] = \frac{k}{2} \cdot 2 = k\). 
    \item \(\var[X] = \frac{k}{2} \cdot 2^2 = 2k\). 
    \item \(\mathscr{M}_X (t) = {(1-2t)}^{-\sfrac{k}{2}}, t<\frac{1}{2}\). 
\end{enumerate}
In practice, \(k\) is usually some positive integer, but it could theoretically take on any value. 

\begin{theorem}\label{theo:1.2.1}
    If \(Z\sim \mathscr{N}(0,1)\), then \(Z^2 \sim \upchi_1^2\). 
    Similarly, if \((Z_i)_{i=1}^n \iid \mathscr{N}(0,1)\), then \(\sum Z_i^2 \sim \upchi_n^2\). 
\end{theorem}

\begin{proof}
    We will use mgf's. Recall that \(\int_{-\infty}^\infty f(x) \der x = \sqrt{2\uppi\sigma^2}\) when \(f\) is of the form \(\exp \big(-\frac{{(x-\mu)}^2}{2\sigma^2}\big)\), i.e., a gaussian kernel.\begin{align*}
        \mathscr{M}_{Z^2}(t) &= \E\big[\e^{tZ^2}\big] = \int_{-\infty} ^\infty \e^{tz^2} \cdot \frac{1}{\sqrt{2\uppi}}\e^{-\sfrac{z^2}{2}} \der z = \frac{1}{\sqrt{2\uppi}} \int_{-\infty}^\infty \exp\biggl(-\frac{z^2}{2} + tz^2\biggr) \der z \\ 
        &= \frac{1}{\sqrt{2\uppi}} \int_{-\infty}^\infty \exp\bigg[-\frac{1}{2}(1 - 2t)z^2\bigg] \der z = \frac{1}{\sqrt{2\uppi}} \int_{-\infty}^\infty \underbrace{\exp\bigg[-\frac{1}{2} \bigg(\frac{{(z-0)}^2}{{(1-2t)}^{-1}}\bigg)\bigg]\der z}_{\text{gaussian kernel of}~\mathscr{N}(0, \frac{1}{1-2t})} \\ 
        &= \frac{1}{\sqrt{2\uppi}} \cdot \sqrt{\frac{2\uppi}{1-2t}} = {(1-2t)}^{-\sfrac{1}{2}} = \mathscr{M}_{\upchi_1^2}(t), t<\frac{1}{2}.
    \end{align*}The proof of the second statement follows by simply multiplying \(n\) of these mgf's together. 
\end{proof}

\begin{minipage}{.14\textwidth}
    \includegraphics[width=2cm]{nerd_maddy.png} 
\end{minipage}%
\fbox{
\begin{minipage}[t]{.76\textwidth}
    \textbf{Nerd Interjection!} What the hell is a ``Gaussian kernel''?! Well, we know that\[
    \int_{-\infty}^\infty \frac{1}{\sqrt{2\uppi \sigma^2}} \e^{-\frac{1}{2}{\big(\frac{x-\mu}{\sigma}\big)}^2} \der x = 1
    \]since it's a valid pdf, so multiplying both sides by \(\sqrt{2\uppi \sigma^2}\) gives us\[
        \int_{-\infty}^\infty\e^{-\frac{1}{2}{\big(\frac{x-\mu}{\sigma}\big)}^2} \der x = \sqrt{2\uppi \sigma^2},
    \]and this holds for any value of \(\mu\) and \(\sigma^2\). 
    Keep this sort of thing in mind, as it shows up a lot and we'll be using this technique with the gamma distribution as well later. 
\end{minipage}}

A corollary of this is that \(\upchi_n^2 \approach \mathscr{N}(n,2n)\) by an application of the CLT. 
We can now prove our main result: 

\begin{theorem}[Distribution of the sample variance]
    Let \((X_i)_{i=1}^n \iid \mathscr{N}(\mu, \sigma^2)\). Then,\[
    \frac{(n-1)S_n^2}{\sigma^2} \sim \upchi_{n-1}^2.
    \]
\end{theorem}

\begin{proof}
    We begin by expanding the definition of \(S_n^2\) to turn it into a more workable form.\begin{align*}
        \sum_{i=1}^n {(X_i - \overline{X}_n)}^2 &= \sum_{i=1}^n {(X_i - \mu + \mu - \overline{X}_n)}^2 = \sum_{i=1}^n {[(X_i - \mu) - (\overline{X}_n -\mu)]}^2 \\ 
        &= \sum_{i=1}^n \big[{(X_i -\mu)}^2  - 2(X_i - \mu)(\overline{X}_n -\mu) + {(\overline{X}_n - \mu)}^2\big] \\ 
        &= \sum_{i=1}^n {(X_i -\mu)}^2 - 2 \sum_{i=1}^n (X_i - \mu)(\overline{X}_n -\mu) + \sum_{i=1}^n {(\overline{X}_n - \mu)}^2 \\ 
        &= \sum_{i=1}^n {(X_i -\mu)}^2 - 2(\overline{X}_n - \mu )\bigg(\sum_{i=1}^{n} X_i - n\mu\bigg) + n{(\overline{X}_n -\mu)}^2 \\ 
        &= \sum_{i=1}^n {(X_i -\mu)}^2 - 2n(\overline{X}_n - \mu )(\overline{X}_n - \mu ) + n{(\overline{X}_n -\mu)}^2 =  \sum_{i=1}^n {(X_i -\mu)}^2 - n{(\overline{X}_n -\mu)}^2.
    \end{align*}Put more neatly, this gives us\begin{equation}\label{eq:2-1}
         \sum_{i=1}^n  {(X_i - \overline{X}_n)}^2 = \sum_{i=1}^n {(X_i - \mu)}^2 - n{(\overline{X}_n - \mu)}^2.
    \end{equation} 
    But \(\frac{X_i - \mu}{\sigma} \sim \mathscr{N}(0,1)\), so we can apply thm.~\ref{theo:1.2.1}~of this section to obtain \(
    \sum_{i=1}^n {\big(\frac{X_i - \mu}{\sigma}\big)}^2 \sim \upchi_n^2
    \). Finally, dividing both sides of equation~\ref{eq:2-1}~by \(\sigma^2\),\begin{align*}
        \frac{1}{\sigma^2}\sum_{i=1}^{n} {(X_i - \overline{X}_n)}^2&=  \sum_{i=1}^n {\bigg(\frac{X_i -\mu}{\sigma}\bigg)}^2 - n{\bigg(\frac{\overline{X}_n -\mu}{\sigma}\bigg)}^2 \\ 
        \frac{n-1}{\sigma^2} \underbrace{\frac{1}{n-1}\sum_{i=1}^{n} {(X_i - \overline{X}_n)}^2}_{S_n^2}&=  \sum_{i=1}^n {\bigg(\frac{X_i -\mu}{\sigma}\bigg)}^2- {\bigg(\frac{\overline{X}_n -\mu}{\sigma/\sqrt{n}}\bigg)}^2 \\ 
        \frac{n-1}{\sigma^2} S_n^2 &= \underbrace{\sum_{i=1}^n {\bigg(\frac{X_i -\mu}{\sigma}\bigg)}^2}_{\sim\,\upchi_n^2}  -  \underbrace{{\bigg(\frac{\overline{X}_n -\mu}{\sigma/\sqrt{n}}\bigg)}^2}_{\sim\,\upchi_1^2} &\text{since}~\frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}}\sim \mathscr{N}(0,1) \\ 
        \mathscr{M}_{\frac{n-1}{\sigma^2}S_n^2}(t) & = \frac{\mathscr{M}_{\upchi_n^2}(t)}{\mathscr{M}_{\upchi_1^2}(t)} = \frac{{(1-2t)}^{-\sfrac{n}{2}}}{{(1-2t)}^{-\sfrac{1}{2}}} = {(1-2t)}^{-\frac{n-1}{2}}, &\text{taking mgf's}
    \end{align*}which is the mgf of \(\upchi_{n-1}^2\), so \(\frac{n-1}{\sigma^2}S_n^2 \sim \upchi_{n-1}^2\). 
\end{proof}

Note that for the above trick with the mgf's to work as intended, we need \(\overline{X}_n\) and \(S_n^2\) to be independent. 
They \textit{are} independent, but this will be proven in \textsc{math} 62.2. 

\begin{example}
    Let \({(X_i)}_{i=1}^{10} \iid \mathscr{N}(\mu, 4)\). Find the value of \(\Pr[3.6 < S^2 <4.2]\). 

    We first scale the values to fit our theorem. We then have\[
    \Pr\bigg[\frac{9}{4} \cdot 3.6 < \frac{10-1}{4} S^2 < \frac{9}{4}\cdot 4.2\bigg] = \Pr[8.1 < \upchi_9^2 < 9.45] = \code{chisq(9.45,9)} - \code{chisq(8.1,9)} \approx 0.1272876,
    \]as desired. Suppose we wanted to find \(a,b\in\mathbb{R}\) such that \(\Pr[a<S^2<b] = 0.95\). 
    Then, similarly, we scale the values to obtain \(\Pr[\frac{9}{4} a < \upchi_9^2 <\frac{9}{4} b]\). 
    We can state this equivalently as \(\Pr[\upchi_9^2 < \frac{9}{4}a] = 0.025\) and \(\Pr[\upchi_9^2 < \frac{9}{4} ] = 0.975\), which gives us\begin{align*}
        \frac{9}{4}\cdot a &= \code{chisq(0.025, 9)} & \frac{9}{4}\cdot b &= \code{chisq(0.975,9)} \\ 
        a & = 1.200 & b&= 8.4545,
    \end{align*}as desired. 
\end{example}

\section{The student \(t\) and the \(\mathscr{F}\) distributions}